# local-rag-indexer

Локальный RAG на базе FAISS + Ollama (embeddings & generation).

## Быстрый старт

### 1. Подготовка окружения
```bash
python -m venv .venv
# На Mac/Linux:
source .venv/bin/activate
# На Windows:
.venv\Scripts\activate

pip install -e .
```

### 2. Запуск сервиса Ollama
Теперь сервис можно запустить прямо из проекта:
```bash
python -m rag ensure-ollama
```
*Для работы эмбеддингов по умолчанию требуется модель `mxbai-embed-large`. Подтяните её вручную, если она отсутствует: `ollama pull mxbai-embed-large`.*

## Команды

### Инжест корпуса
Чтение файлов (`.md`, `.txt`, `.py`) и создание векторного индекса.
```bash
python -m rag ingest --corpus ./corpus --store ./store
```

### Поиск (Векторный)
Простой поиск похожих чанков по базе.
```bash
python -m rag search "ваш запрос" --threshold 0.5 --k 3
```
*   `--threshold`: Минимум схожести (0.0 - 1.0). Позволяет отсечь шум.
*   `--k`: Количество возвращаемых результатов.

### Вопрос агенту (RAG)
Генерация ответа с использованием извлеченного контекста.
```bash
python -m rag ask "Ваш вопрос" --mode rag --rerank --threshold 0.4 -v
```
*   `--mode`: `rag` (только RAG), `no-rag` (только LLM), `compare` (сравнение результатов судьей).
*   `--rerank`: **Использовать LLM-реранкер**. Модель проверит каждый чанк на реальную релевантность вопросу перед генерацией.
*   `--threshold`: Предварительная фильтрация по вектору.
*   `-v`: Показать извлеченные чанки.

## Технические детали
*   **Индекс:** FAISS `IndexFlatIP` с L2-нормировкой.
*   **Реранкер:** Встроенный в `Agent` алгоритм фильтрации на базе LLM (второй этап после векторного поиска).
*   **Чанкинг:** Фиксированный размер с перекрытием (конфигурируется в `ingest`).

